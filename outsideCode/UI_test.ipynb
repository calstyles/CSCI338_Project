{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f3827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 363069 images under train\n",
      "Loaded 11932 images under val\n",
      "Loaded 11965 images under test\n",
      "Classes: \n",
      "['bar', 'line', 'pie']\n",
      "Doing eval now --------------------\n",
      "Running on local URL:  http://127.0.0.1:7890/\n",
      "Running on public URL: https://12066.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x2ca1ceb6048>",
      "text/html": "\n        <iframe\n            width=\"900\"\n            height=\"500\"\n            src=\"https://12066.gradio.app\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "def predict(inp):\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_transforms = transforms.Compose([\n",
    "           transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "    \n",
    "    inp = data_transforms(inp).unsqueeze(0)\n",
    "    outputs = vgg16(inp)\n",
    "    x, preds = torch.max(outputs.data, 1)\n",
    "    labels = [preds[j] for j in range(inp.size()[0])]\n",
    "    predicted_labels = str(labels)\n",
    "    ans = \"\"\n",
    "        \n",
    "    if(predicted_labels == \"[tensor(1)]\"):\n",
    "        ans = \"Line graph\"\n",
    "    elif(predicted_labels == \"[tensor(2)]\"):\n",
    "        ans = \"Pie graph\"\n",
    "    else:\n",
    "        ans = \"Bar graph\"\n",
    "    \n",
    "    return ans\n",
    "\n",
    "\n",
    "def main():\n",
    "  pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #Check to see if CUDA is availble on the running systems.\n",
    "#     use_gpu = torch.cuda.is_available()\n",
    "#     if use_gpu:\n",
    "#         print(\"Using CUDA\")\n",
    "\n",
    "    # Loading this dataset with pytorch is really easy using ImageFolder\n",
    "    # as the labels are specified by the folders names.\n",
    "    # This folder path will be custom for each member\n",
    "    data_dir = r'D:\\School\\SE\\Group_Project\\data'\n",
    "    TRAIN = 'train'\n",
    "    VAL = 'val'\n",
    "    TEST = 'test'\n",
    "\n",
    "    # VGG-16 Takes 224x224 images as input, so we resize all of them\n",
    "    #it also takes normalization as specified at pytorch.com\n",
    "    data_transforms = {\n",
    "        TRAIN: transforms.Compose([\n",
    "            # Data augmentation is a good practice for the train set\n",
    "            # Here, we randomly crop the image to 224x224 and\n",
    "            # randomly flip it horizontally (seems like best fit for graphs?)\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ]),\n",
    "        VAL: transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ]),\n",
    "        TEST: transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    #creating our data set\n",
    "    image_datasets = {\n",
    "        x: datasets.ImageFolder(\n",
    "            os.path.join(data_dir, x),\n",
    "            transform=data_transforms[x]\n",
    "        )\n",
    "        for x in [TRAIN, VAL, TEST]\n",
    "    }\n",
    "\n",
    "    #low batch sizes allow for our computers to compute these predicitions as they don't use much\n",
    "    #video memory\n",
    "    dataloaders = {\n",
    "        x: torch.utils.data.DataLoader(\n",
    "            image_datasets[x], batch_size=8,\n",
    "            shuffle=True, num_workers=4\n",
    "        )\n",
    "        for x in [TRAIN, VAL, TEST]\n",
    "    }\n",
    "\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}\n",
    "\n",
    "    for x in [TRAIN, VAL, TEST]:\n",
    "        print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
    "\n",
    "\n",
    "    print(\"Classes: \")\n",
    "    class_names = image_datasets[TRAIN].classes\n",
    "    print(image_datasets[TRAIN].classes)\n",
    "\n",
    "\n",
    "    \n",
    "    def imshow(inp, title=None):\n",
    "        inp = inp.numpy().transpose((1, 2, 0))\n",
    "        # plt.figure(figsize=(10, 10))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(inp)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.pause(0.001)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def show_databatch(inputs, classes):\n",
    "        out = torchvision.utils.make_grid(inputs)\n",
    "        imshow(out, title=[class_names[x] for x in classes])\n",
    "    \n",
    "    \n",
    "    inputs, classes = next(iter(dataloaders[TRAIN]))\n",
    "    #show_databatch(inputs, classes)\n",
    "    \n",
    "    #allows the itegration of matplot lib to see some data\n",
    "#     def visualize_model(vgg, num_images=25):\n",
    "#         #was_training = vgg.training\n",
    "    \n",
    "#         # Set model for evaluation\n",
    "#         #vgg.train(False)\n",
    "#         vgg.eval()\n",
    "    \n",
    "#         images_so_far = 0\n",
    "    \n",
    "#         for i, data in enumerate(dataloaders[TEST]):\n",
    "#             inputs, labels = data\n",
    "#             size = inputs.size()[0]\n",
    "    \n",
    "#             #if use_gpu:\n",
    "#                 #inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n",
    "#            # else:\n",
    "#             inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "    \n",
    "#             outputs = vgg(inputs)\n",
    "    \n",
    "#             _, preds = torch.max(outputs.data, 1)\n",
    "#             predicted_labels = [preds[j] for j in range(inputs.size()[0])]\n",
    "    \n",
    "#             print(\"Ground truth:\")\n",
    "#             show_databatch(inputs.data.cpu(), labels.data.cpu())\n",
    "#             print(\"Prediction:\")\n",
    "#             show_databatch(inputs.data.cpu(), predicted_labels)\n",
    "    \n",
    "#             del inputs, labels, outputs, preds, predicted_labels\n",
    "#             torch.cuda.empty_cache()\n",
    "    \n",
    "#             images_so_far += size\n",
    "#             if images_so_far >= num_images:\n",
    "#                 break\n",
    "    \n",
    "#         #vgg.train(mode=was_training)  # Revert model back to original training state\n",
    "    \n",
    "    \n",
    "    # # Load the pretrained model from pytorch\n",
    "    vgg16 = models.vgg16_bn()\n",
    "\n",
    "    #print(vgg16.classifier[6].out_features)  # 1000\n",
    "    \n",
    "    # Freeze training for all layers\n",
    "    for param in vgg16.features.parameters():\n",
    "        param.require_grad = False\n",
    "    \n",
    "    # Newly created modules have require_grad=True by default\n",
    "    num_features = vgg16.classifier[6].in_features\n",
    "    features = list(vgg16.classifier.children())[:-1]  # Remove last layer\n",
    "    features.extend([nn.Linear(num_features, len(class_names))])  # Add our layer with 4 outputs\n",
    "    vgg16.classifier = nn.Sequential(*features)  # Replace the model classifier\n",
    "    #print(vgg16)\n",
    "\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    #optimization (possibly could use atom)\n",
    "    optimizer_ft = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "    \n",
    "    \n",
    "    #load trained model\n",
    "    vgg16.load_state_dict(torch.load(r\"C:\\Users\\matth\\PycharmProjects\\CSCI338_Project\\VGG16_graphs.pt\"))\n",
    "    \n",
    "    \n",
    "    print('Doing eval now --------------------')\n",
    "    vgg16.eval()\n",
    "    \n",
    "\n",
    "    gr.Interface(fn=predict, \n",
    "             inputs=gr.inputs.Image(type=\"pil\"),\n",
    "             outputs=gr.outputs.Label(num_top_classes=3)).launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ebf439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a237776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1d3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42bb766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}